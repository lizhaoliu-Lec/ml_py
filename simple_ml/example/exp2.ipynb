{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment-2: Logistic Regression and SVM\n",
    "## In this experiment, we will: \n",
    "- (1) Get more intuitions about Logistic Regression and SVM. \n",
    "- (2) Get your hands dirty on [a9a dataset](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a9a), which is one of the [LIBSVM Dataset](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/).\n",
    "- (3) Get some experience about hyper-parameter tuning, loss function selection in SVM, optimizer selection, initilizer selection.\n",
    "### As usual, in this experiment, we will use the mini framework [simple_ml](https://github.com/lizhaoliu-Lec/simple_ml), which means simple machine learning, written by [lizhaoliu-Lec](https://github.com/lizhaoliu-Lec/)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Requirement already satisfied: simple_ml in e:\\anaconda3\\envs\\ml_py\\lib\\site-packages (0.1.1)",
      "\n",
      "Requirement already satisfied: scipy in e:\\anaconda3\\envs\\ml_py\\lib\\site-packages (from simple_ml) (1.3.1)",
      "\n",
      "Requirement already satisfied: numpy in e:\\anaconda3\\envs\\ml_py\\lib\\site-packages (from simple_ml) (1.17.2)",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.",
      "\n",
      "The autoreload extension is already loaded. To reload it, use:",
      "\n",
      "  %reload_ext",
      " ",
      "autoreload",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# as usual, do a little setup\n",
    "%pip install simple_ml\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from simple_ml.utils.distance import euclidean\n",
    "from simple_ml.preprocessing.general import Standardizer\n",
    "from simple_ml.nn.layer import Input, Linear, Dropout, Softmax\n",
    "from simple_ml.nn.model import Model\n",
    "from simple_ml.nn.initializer import zeros, ones\n",
    "from simple_ml.nn.regularizer import L2_Regularizer, L1_Regularizer, L1L2_Regularizer\n",
    "from simple_ml.nn.optimizer import SGD, Momentum, Adam, RMSProp\n",
    "from simple_ml.utils.metric import accuracy, absolute_error, square_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "We have `32561` training examples and `16281` test examples",
      "\n",
      "X_train shape: ",
      " ",
      "(32561, 123)",
      " ",
      ", X_test shape: ",
      " ",
      "(16281, 123)",
      "\n",
      "y_train shape: ",
      " ",
      "(32561, 1)",
      " ",
      ", y_test shape: ",
      " ",
      "(16281, 1)",
      "\n",
      "But we set aside 20% of the training examples for validation.\n",
      "\n",
      "Finally, we have...",
      "\n",
      "We have `26049` training examples, `6512` validation examples and `16281` test examples",
      "\n",
      "X_train shape: ",
      " ",
      "(26049, 123)",
      " ",
      ", X_val shape: ",
      " ",
      "(6512, 123)",
      " ",
      ", X_test shape: ",
      " ",
      "(16281, 123)",
      "\n",
      "y_train shape: ",
      " ",
      "(26049, 1)",
      " ",
      ", y_val shape: ",
      " ",
      "(6512, 1)",
      " ",
      ", y_test shape: ",
      " ",
      "(16281, 1)",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Load the (preprocessed) a9a data.\n",
    "# fix random seed\n",
    "np.random.seed(1234)\n",
    "\n",
    "# some utils func\n",
    "def convert_to_onehot(x):\n",
    "    return np.array(x == 1, dtype=np.float)\n",
    "\n",
    "# read the data\n",
    "def get_a9a_data(data_path='./tmp/exp2/a9a'):\n",
    "    X_train, y_train = load_svmlight_file(data_path)\n",
    "    X_test, y_test = load_svmlight_file(data_path + '.t', n_features=123)\n",
    "    X_train, X_test = X_train.A, X_test.A\n",
    "    y_train, y_test = np.reshape(y_train, (-1, 1)), np.reshape(y_test, (-1, 1))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = get_a9a_data()\n",
    "print('We have `%d` training examples and `%d` test examples' % (X_train.shape[0], X_test.shape[0]))\n",
    "print('X_train shape: ', X_train.shape, ', X_test shape: ', X_test.shape)\n",
    "print('y_train shape: ', y_train.shape, ', y_test shape: ', y_test.shape)\n",
    "print('But we set aside 20% of the training examples for validation.\\n')\n",
    "val_split = 0.2\n",
    "val_size = int(X_train.shape[0] * val_split)\n",
    "X_val, y_val, X_train, y_train = X_train[:val_size], y_train[:val_size], X_train[val_size:], y_train[val_size:]\n",
    "print('Finally, we have...')\n",
    "print('We have `%d` training examples, `%d` validation examples and `%d` test examples' % (X_train.shape[0], X_val.shape[0],X_test.shape[0]))\n",
    "print('X_train shape: ', X_train.shape, ', X_val shape: ', X_val.shape, ', X_test shape: ', X_test.shape)\n",
    "print('y_train shape: ', y_train.shape, ', y_val shape: ', y_val.shape, ', y_test shape: ', y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalized the data:\n",
    "As usual, we normalize the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Before normalized the data...",
      "\n",
      "*** X_train *** \nmean: \n[1.96514262e-01 1.80467580e-01 2.11601213e-01 1.94709970e-01\n 2.16706975e-01 6.98337748e-01 7.77764981e-02 3.35905409e-02\n 2.98667895e-02 6.43402818e-02 4.01550923e-02 4.99059465e-04\n 2.30335138e-04 2.00122845e-01 2.01428078e-01 1.99086337e-01\n 2.02042305e-01 1.97320435e-01 1.64804791e-01 2.22810856e-01\n 3.57787247e-02 3.22392414e-01 1.79661407e-02 3.29763139e-02\n 4.23432761e-02 1.54708434e-02 1.99239894e-02 1.38584974e-02\n 5.29003033e-02 5.22092979e-03 2.83696111e-02 1.30523245e-02\n 1.04802488e-02 1.65073515e-03 1.30753580e-01 3.22392414e-01\n 2.22810856e-01 7.53195900e-02 2.48723559e-01 4.60670275e-01\n 1.36550347e-01 3.27344620e-01 3.15175247e-02 3.05961841e-02\n 1.26300434e-02 6.91005413e-04 2.86767246e-02 1.25993320e-01\n 1.01501017e-01 1.10445698e-01 1.25225536e-01 1.27720834e-01\n 4.29958924e-02 6.15378709e-02 1.15781796e-01 3.08265193e-02\n 4.86774924e-02 4.76025951e-03 2.01543245e-02 2.68724327e-04\n 4.75642059e-02 1.55130715e-01 4.06196015e-01 2.54597105e-01\n 3.00971246e-02 1.06414834e-01 8.54428193e-01 3.24004760e-02\n 9.36696226e-03 8.63756766e-03 9.51668010e-02 3.31951323e-01\n 6.68048677e-01 9.16503513e-01 8.34964874e-02 9.53779416e-01\n 4.62205843e-02 1.71906791e-01 6.70275250e-02 4.67618719e-01\n 7.93120657e-02 2.14134900e-01 8.96349188e-01 5.37448654e-04\n 2.68724327e-03 3.41663787e-03 3.30147031e-03 4.10764329e-03\n 4.22281086e-04 3.26308112e-03 1.88107029e-03 6.91005413e-04\n 2.53368651e-03 2.14979462e-03 2.91757841e-03 1.11328650e-03\n 3.83891896e-04 6.41099466e-03 2.30335138e-03 1.72751353e-03\n 2.38012976e-03 2.07301624e-03 1.96552651e-02 1.11328650e-03\n 7.29394602e-04 9.98118930e-04 2.11140543e-03 5.75837844e-04\n 9.21340551e-04 1.49717839e-03 1.38201083e-03 2.03462705e-03\n 4.99059465e-04 1.99623786e-03 1.11328650e-03 3.83891896e-04\n 6.14227034e-04 4.99059465e-04 3.30147031e-03 5.75837844e-04\n 1.07489731e-03 7.29394602e-04 3.83891896e-05] \nvariance: \n[1.57896407e-01 1.47899033e-01 1.66826140e-01 1.56797997e-01\n 1.69745062e-01 2.10662138e-01 7.17273145e-02 3.24622165e-02\n 2.89747644e-02 6.02006099e-02 3.85426609e-02 4.98810405e-04\n 2.30282083e-04 1.60073692e-01 1.60854807e-01 1.59450968e-01\n 1.61221212e-01 1.58385081e-01 1.37644172e-01 1.73166179e-01\n 3.44986076e-02 2.18455546e-01 1.76433585e-02 3.18888766e-02\n 4.05503231e-02 1.52314964e-02 1.95270241e-02 1.36664395e-02\n 5.01018612e-02 5.19367168e-03 2.75647763e-02 1.28819613e-02\n 1.03704131e-02 1.64801023e-03 1.13657081e-01 2.18455546e-01\n 1.73166179e-01 6.96465494e-02 1.86860150e-01 2.48453173e-01\n 1.17904350e-01 2.20190120e-01 3.05241703e-02 2.96600576e-02\n 1.24705254e-02 6.90527924e-04 2.78543701e-02 1.10119004e-01\n 9.11985608e-02 9.82474462e-02 1.09544101e-01 1.11408222e-01\n 4.11472456e-02 5.77509614e-02 1.02376372e-01 2.98762450e-02\n 4.63079942e-02 4.73759944e-03 1.97481277e-02 2.68652114e-04\n 4.53018522e-02 1.31065176e-01 2.41200812e-01 1.89777419e-01\n 2.91912877e-02 9.50907168e-02 1.24380656e-01 3.13506852e-02\n 9.27922228e-03 8.56296009e-03 8.61100810e-02 2.21759642e-01\n 2.21759642e-01 7.65248240e-02 7.65248240e-02 4.40842419e-02\n 4.40842419e-02 1.42354846e-01 6.25348359e-02 2.48951453e-01\n 7.30216620e-02 1.68281144e-01 9.29073211e-02 5.37159803e-04\n 2.68002200e-03 3.40496446e-03 3.29057060e-03 4.09077055e-03\n 4.22102764e-04 3.25243342e-03 1.87753187e-03 6.90527924e-04\n 2.52726695e-03 2.14517300e-03 2.90906615e-03 1.11204709e-03\n 3.83744523e-04 6.36989381e-03 2.29804595e-03 1.72452923e-03\n 2.37446474e-03 2.06871884e-03 1.92689356e-02 1.11204709e-03\n 7.28862586e-04 9.97122688e-04 2.10694740e-03 5.75506255e-04\n 9.20491682e-04 1.49493685e-03 1.38010087e-03 2.03048734e-03\n 4.98810405e-04 1.99225289e-03 1.11204709e-03 3.83744523e-04\n 6.13849759e-04 4.98810405e-04 3.29057060e-03 5.75506255e-04\n 1.07374190e-03 7.28862586e-04 3.83877159e-05]",
      "\n",
      "*** X_val *** \nmean: \n[1.98402948e-01 1.80589681e-01 2.02395577e-01 2.01013514e-01\n 2.17598280e-01 6.91799754e-01 7.90847666e-02 3.70085995e-02\n 2.79484029e-02 6.40356265e-02 3.86977887e-02 1.53562654e-04\n 1.53562654e-04 1.99324324e-01 1.94410319e-01 2.03624079e-01\n 1.91799754e-01 2.10841523e-01 1.63083538e-01 2.28347666e-01\n 3.73157248e-02 3.22942260e-01 1.65847666e-02 3.19410319e-02\n 4.28439803e-02 1.70454545e-02 1.95024570e-02 1.10565111e-02\n 5.29791155e-02 4.91400491e-03 2.97911548e-02 1.12100737e-02\n 9.21375921e-03 1.22850123e-03 1.30067568e-01 3.22942260e-01\n 2.28347666e-01 7.47850123e-02 2.43857494e-01 4.57002457e-01\n 1.36056511e-01 3.31081081e-01 3.13267813e-02 3.00982801e-02\n 1.36670762e-02 7.67813268e-04 2.77948403e-02 1.25460688e-01\n 9.99692875e-02 1.18703931e-01 1.23464373e-01 1.24846437e-01\n 3.83906634e-02 6.12714988e-02 1.15786241e-01 2.93304668e-02\n 5.05221130e-02 3.83906634e-03 1.90417690e-02 3.07125307e-04\n 5.05221130e-02 1.57708845e-01 4.01105651e-01 2.56910319e-01\n 3.02518428e-02 1.03501229e-01 8.53654791e-01 2.99447174e-02\n 1.02886978e-02 7.06388206e-03 9.90479115e-02 3.26167076e-01\n 6.73832924e-01 9.17536855e-01 8.24631450e-02 9.51627764e-01\n 4.83722359e-02 1.69686732e-01 6.66461916e-02 4.66216216e-01\n 7.78562654e-02 2.19594595e-01 8.93888206e-01 7.67813268e-04\n 3.07125307e-03 3.83906634e-03 5.37469287e-03 4.60687961e-03\n 4.60687961e-04 2.30343980e-03 1.99631450e-03 1.68918919e-03\n 2.14987715e-03 2.91769042e-03 2.91769042e-03 2.14987715e-03\n 4.60687961e-04 4.76044226e-03 1.99631450e-03 2.30343980e-03\n 2.91769042e-03 1.99631450e-03 2.01167076e-02 1.22850123e-03\n 7.67813268e-04 4.60687961e-04 2.30343980e-03 4.60687961e-04\n 6.14250614e-04 1.84275184e-03 1.22850123e-03 9.21375921e-04\n 0.00000000e+00 1.84275184e-03 7.67813268e-04 3.07125307e-04\n 3.07125307e-04 4.60687961e-04 3.07125307e-03 6.14250614e-04\n 4.60687961e-04 1.53562654e-04 0.00000000e+00] \nvariance: \n[1.59039218e-01 1.47977048e-01 1.61431608e-01 1.60607081e-01\n 1.70249269e-01 2.13212854e-01 7.28303663e-02 3.56389631e-02\n 2.71672897e-02 5.99350651e-02 3.72002698e-02 1.53539072e-04\n 1.53539072e-04 1.59594138e-01 1.56614947e-01 1.62161313e-01\n 1.55012609e-01 1.66387375e-01 1.36487298e-01 1.76205009e-01\n 3.59232615e-02 2.18650557e-01 1.63097121e-02 3.09208024e-02\n 4.10083737e-02 1.67549070e-02 1.91221112e-02 1.09342646e-02\n 5.01723288e-02 4.88985747e-03 2.89036419e-02 1.10844080e-02\n 9.12886585e-03 1.22699201e-03 1.13149995e-01 2.18650557e-01\n 1.76205009e-01 6.91922142e-02 1.84391017e-01 2.48151211e-01\n 1.17545137e-01 2.21466399e-01 3.03454141e-02 2.91923736e-02\n 1.34802872e-02 7.67223731e-04 2.70222871e-02 1.09720304e-01\n 8.99754290e-02 1.04613308e-01 1.08220922e-01 1.09259804e-01\n 3.69168204e-02 5.75173022e-02 1.02379787e-01 2.84701905e-02\n 4.79696291e-02 3.82432791e-03 1.86791801e-02 3.07030981e-04\n 4.79696291e-02 1.32836765e-01 2.40219908e-01 1.90907407e-01\n 2.93366688e-02 9.27887242e-02 1.24928289e-01 2.90480313e-02\n 1.01828405e-02 7.01398363e-03 8.92374228e-02 2.19782115e-01\n 2.19782115e-01 7.56629747e-02 7.56629747e-02 4.60323627e-02\n 4.60323627e-02 1.40893145e-01 6.22044768e-02 2.48858656e-01\n 7.17946673e-02 1.71372809e-01 9.48520809e-02 7.67223731e-04\n 3.06182048e-03 3.82432791e-03 5.34580555e-03 4.58565627e-03\n 4.60475727e-04 2.29813397e-03 1.99232922e-03 1.68633583e-03\n 2.14525518e-03 2.90917750e-03 2.90917750e-03 2.14525518e-03\n 4.60475727e-04 4.73778045e-03 1.99232922e-03 2.29813397e-03\n 2.90917750e-03 1.99232922e-03 1.97120257e-02 1.22699201e-03\n 7.67223731e-04 4.60475727e-04 2.29813397e-03 4.60475727e-04\n 6.13873310e-04 1.83935611e-03 1.22699201e-03 9.20526988e-04\n 0.00000000e+00 1.83935611e-03 7.67223731e-04 3.07030981e-04\n 3.07030981e-04 4.60475727e-04 3.06182048e-03 6.13873310e-04\n 4.60475727e-04 1.53539072e-04 0.00000000e+00]",
      "\n",
      "*** X_test *** \nmean: \n[1.97530864e-01 1.76094834e-01 2.10675020e-01 1.93845587e-01\n 2.21853694e-01 6.88532645e-01 8.11375223e-02 3.55629261e-02\n 2.89908482e-02 6.40624040e-02 4.19507401e-02 4.29949020e-04\n 1.84263866e-04 2.05822738e-01 2.00540507e-01 1.94029851e-01\n 1.98145077e-01 2.01461827e-01 1.63994841e-01 2.20318162e-01\n 3.91253609e-02 3.24488668e-01 1.58466925e-02 3.27989681e-02\n 4.17050550e-02 1.48639518e-02 1.89791782e-02 1.37583687e-02\n 5.73674836e-02 4.85228180e-03 2.80081076e-02 1.11172532e-02\n 1.08101468e-02 1.96548124e-03 1.32362877e-01 3.24488668e-01\n 2.20318162e-01 7.45040231e-02 2.48326270e-01 4.54701800e-01\n 1.34512622e-01 3.33763282e-01 3.10177508e-02 3.22461765e-02\n 1.28984706e-02 8.59898041e-04 3.18162275e-02 1.23641054e-01\n 9.99938579e-02 1.13875069e-01 1.24071003e-01 1.24808058e-01\n 4.31177446e-02 6.26497144e-02 1.13076592e-01 3.04649592e-02\n 4.65573368e-02 5.71217984e-03 2.05147104e-02 3.68527732e-04\n 4.68644432e-02 1.54351698e-01 4.00651066e-01 2.62760273e-01\n 3.22461765e-02 1.03126344e-01 8.56581291e-01 2.94822185e-02\n 9.76598489e-03 8.29187396e-03 9.58786315e-02 3.32964806e-01\n 6.67035194e-01 9.18739635e-01 8.12603648e-02 9.53135557e-01\n 4.68644432e-02 1.72716664e-01 6.83004729e-02 4.65941895e-01\n 7.91720410e-02 2.13868927e-01 9.00558934e-01 5.52791598e-04\n 2.27258768e-03 4.29949020e-03 3.74669861e-03 4.23806891e-03\n 5.52791598e-04 3.13248572e-03 1.84263866e-03 1.22842577e-03\n 2.14974510e-03 2.88680057e-03 2.64111541e-03 9.82740618e-04\n 4.29949020e-04 5.95786500e-03 1.96548124e-03 1.65837479e-03\n 1.53553222e-03 1.16700448e-03 1.89177569e-02 1.84263866e-03\n 7.98476752e-04 5.52791598e-04 2.02690252e-03 3.07106443e-04\n 1.04416191e-03 8.59898041e-04 1.90405995e-03 1.59695350e-03\n 3.68527732e-04 1.47411093e-03 9.21319329e-04 5.52791598e-04\n 7.37055463e-04 4.29949020e-04 3.00964314e-03 4.91370309e-04\n 9.21319329e-04 6.14212886e-04 0.00000000e+00] \nvariance: \n[1.58512422e-01 1.45085444e-01 1.66291056e-01 1.56269475e-01\n 1.72634633e-01 2.14455442e-01 7.45542247e-02 3.42982044e-02\n 2.81503789e-02 5.99584124e-02 4.01908755e-02 4.29764164e-04\n 1.84229913e-04 1.63459739e-01 1.60324012e-01 1.56382268e-01\n 1.58883606e-01 1.60874959e-01 1.37100533e-01 1.71778070e-01\n 3.75945670e-02 2.19195772e-01 1.55955748e-02 3.17231958e-02\n 3.99657434e-02 1.46430148e-02 1.86189690e-02 1.35690759e-02\n 5.40764554e-02 4.82873716e-03 2.72236535e-02 1.09936599e-02\n 1.06932875e-02 1.96161812e-03 1.14842946e-01 2.19195772e-01\n 1.71778070e-01 6.89531736e-02 1.86660334e-01 2.47948073e-01\n 1.16418977e-01 2.22365354e-01 3.00556499e-02 3.12063606e-02\n 1.27321001e-02 8.59158616e-04 3.08039552e-02 1.08353944e-01\n 8.99950863e-02 1.00907538e-01 1.08677389e-01 1.09231007e-01\n 4.12586047e-02 5.87247277e-02 1.00290277e-01 2.95368454e-02\n 4.43897512e-02 5.67955084e-03 2.00938571e-02 3.68391919e-04\n 4.46681672e-02 1.30527252e-01 2.40129789e-01 1.93717312e-01\n 3.12063606e-02 9.24913008e-02 1.22849783e-01 2.86130173e-02\n 9.67061043e-03 8.22311879e-03 8.66859195e-02 2.22099244e-01\n 2.22099244e-01 7.46571179e-02 7.46571179e-02 4.46681672e-02\n 4.46681672e-02 1.42885618e-01 6.36355183e-02 2.48840046e-01\n 7.29038289e-02 1.68129009e-01 8.95525406e-02 5.52486019e-04\n 2.26742302e-03 4.28100459e-03 3.73266086e-03 4.22010769e-03\n 5.52486019e-04 3.12267325e-03 1.83924334e-03 1.22691674e-03\n 2.14512370e-03 2.87846695e-03 2.63413992e-03 9.81774839e-04\n 4.29764164e-04 5.92236884e-03 1.96161812e-03 1.65562459e-03\n 1.53317436e-03 1.16564258e-03 1.85598754e-02 1.83924334e-03\n 7.97839187e-04 5.52486019e-04 2.02279419e-03 3.07012129e-04\n 1.04307163e-03 8.59158616e-04 1.90043450e-03 1.59440324e-03\n 3.68391919e-04 1.47193792e-03 9.20470500e-04 5.52486019e-04\n 7.36512213e-04 4.29764164e-04 3.00058519e-03 4.91128864e-04\n 9.20470500e-04 6.13835629e-04 0.00000000e+00]",
      "\n",
      "\nAfter normalized the data...",
      "\n",
      "*** X_train *** \nmean: \n[-8.99379103e-16 -8.01778016e-17 -2.61411513e-15 -9.27636536e-16\n  2.76371331e-15  9.71160654e-16 -7.28688014e-16 -1.11430607e-15\n -4.46358755e-16 -1.63718792e-15 -4.71362108e-17 -1.54547513e-15\n -1.69652586e-15 -1.98703880e-15 -4.00363070e-15 -3.15093816e-16\n  1.02741553e-15 -1.33736505e-15  3.44783300e-16 -1.28909300e-15\n -9.52136966e-16  8.99310911e-16  1.26969212e-15  6.76876327e-16\n -9.22117173e-17 -3.07829781e-15 -3.36553269e-17  2.28322720e-15\n  1.67210908e-15 -6.85236351e-16 -1.49157050e-16  4.76535178e-16\n -3.59364860e-16  8.81082096e-16  1.06344696e-15  8.99310911e-16\n -1.28909300e-15  6.66449207e-16 -4.86616007e-16  2.91533170e-16\n  2.61992858e-16 -1.83336610e-16 -1.83887481e-15  1.94826688e-15\n -6.79662647e-16 -6.22426695e-16 -1.23767982e-16 -1.20409907e-15\n  2.59255765e-15  4.02465969e-17  1.59407722e-15  1.11878976e-17\n  4.98089249e-15  6.84818669e-16 -2.04213014e-15  1.80039484e-15\n  8.17519921e-16  5.76714810e-16  2.34999125e-16 -2.04680029e-15\n -1.06912828e-15 -9.12361327e-16  7.41725644e-16 -2.70254428e-15\n  8.99191573e-16  6.73558317e-16 -2.41301001e-15 -7.73565335e-16\n  5.67437379e-16 -4.12976199e-16  2.95127362e-15 -1.96702419e-16\n  1.96702419e-16  4.83930911e-16 -5.15060970e-16 -7.19705730e-16\n  7.16372802e-16 -1.50488943e-16 -1.29179941e-15 -8.33743438e-16\n -5.72747901e-16  1.37215195e-15 -7.40915853e-16  3.75906166e-16\n -1.64334659e-15 -3.90175051e-15  1.13859766e-16 -1.27888538e-15\n  1.44341460e-16  1.39022414e-16 -3.89304739e-16 -1.23556131e-15\n -1.66106449e-15 -2.09820495e-16 -7.20253405e-16 -7.61787143e-16\n  8.54407751e-16  4.75011493e-16  6.04088399e-16 -2.58502660e-15\n  1.05407097e-15  6.69892948e-16 -3.33250176e-16 -1.01703903e-15\n  7.34325649e-18 -4.97722925e-16 -1.20526475e-15 -3.75084255e-15\n  9.77534826e-16  2.94629553e-16 -5.27874842e-16 -5.54360325e-17\n -1.12353130e-16  2.17060397e-15 -7.60793551e-16  1.03103029e-15\n  4.50319271e-16 -7.85206875e-16 -2.60228526e-15 -2.71042029e-15\n  9.58587056e-16  3.21639695e-15  4.65774618e-16] \nvariance: \n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1.]",
      "\n",
      "*** X_val *** \nmean: \n[ 4.75306580e-03  3.17492831e-04 -2.25383300e-02  1.59189538e-02\n  2.16335435e-03 -1.42446329e-02  4.88488767e-03  1.89710086e-02\n -1.12700584e-02 -1.24167577e-03 -7.42299553e-03 -1.54695005e-02\n -5.05912626e-03 -1.99584314e-03 -1.74977172e-02  1.13638674e-02\n -2.55092110e-02  3.39746138e-02 -4.63944220e-03  1.33054031e-02\n  8.27509739e-03  1.17641255e-03 -1.03996972e-02 -5.79747685e-03\n  2.48647493e-03  1.27585709e-02 -3.01656670e-03 -2.39683606e-02\n  3.52100422e-04 -4.25887445e-03  8.56215274e-03 -1.62314661e-02\n -1.24366548e-02 -1.04009539e-02 -2.03485565e-03  1.17641255e-03\n  1.33054031e-02 -2.02563434e-03 -1.12569208e-02 -7.35843628e-03\n -1.43819626e-03  7.96272244e-03 -1.09176067e-03 -2.89107684e-03\n  9.28645828e-03  2.92290711e-03 -5.28402617e-03 -1.60507888e-03\n -5.07210437e-03  2.63467155e-02 -5.32114459e-03 -8.61168326e-03\n -2.27028742e-02 -1.10843245e-03  1.38920402e-05 -8.65533348e-03\n  8.57193897e-03 -1.33835566e-02 -7.91696493e-03  2.34286452e-03\n  1.38971758e-02  7.12133076e-03 -1.03647651e-02  5.30998795e-03\n  9.05554311e-04 -9.44847298e-03 -2.19295033e-03 -1.38695430e-02\n  9.56864709e-03 -1.70061502e-02  1.32260150e-02 -1.22830303e-02\n  1.22830303e-02  3.73545140e-03 -3.73545140e-03 -1.02477910e-02\n  1.02477910e-02 -5.88407718e-03 -1.52490870e-03 -2.81090563e-03\n -5.38735963e-03  1.33091602e-02 -8.07390085e-03  9.93948801e-03\n  7.41776597e-03  7.23930514e-03  3.61418452e-02  7.80555183e-03\n  1.86938982e-03 -1.68269238e-02  2.65965583e-03  3.79856781e-02\n -7.63466539e-03  1.65794886e-02  2.07668849e-06  3.10846297e-02\n  3.92028707e-03 -2.06806041e-02 -6.40488284e-03  1.38685677e-02\n  1.10317553e-02 -1.68637632e-03  3.32421240e-03  3.45498700e-03\n  1.42304766e-03 -1.70195624e-02  4.18361904e-03 -4.79996951e-03\n -1.01217510e-02  8.93776523e-03 -4.13218843e-03 -2.47054632e-02\n -2.23452154e-02 -3.43871816e-03 -1.03598344e-02 -3.91878240e-03\n -1.23951336e-02 -1.71807086e-03 -4.01330556e-03  1.60121852e-03\n -1.87441811e-02 -2.13291195e-02 -6.19601996e-03] \nvariance: \n[1.00723773e+00 1.00052749e+00 9.67663748e-01 1.02429293e+00\n 1.00297037e+00 1.01210809e+00 1.01537841e+00 1.09785982e+00\n 9.37619004e-01 9.95589001e-01 9.65171293e-01 3.07810484e-01\n 6.66743456e-01 9.97004167e-01 9.73641694e-01 1.01699799e+00\n 9.61490158e-01 1.05052430e+00 9.91595182e-01 1.01754864e+00\n 1.04129598e+00 1.00089268e+00 9.24410853e-01 9.69642262e-01\n 1.01129586e+00 1.10001713e+00 9.79263974e-01 8.00081442e-01\n 1.00140649e+00 9.41503001e-01 1.04857161e+00 8.60459654e-01\n 8.80279862e-01 7.44529369e-01 9.95538459e-01 1.00089268e+00\n 1.01754864e+00 9.93476559e-01 9.86786193e-01 9.98784634e-01\n 9.96953351e-01 1.00579626e+00 9.94143782e-01 9.84231858e-01\n 1.08097187e+00 1.11106836e+00 9.70127382e-01 9.96379373e-01\n 9.86588256e-01 1.06479417e+00 9.87921033e-01 9.80715804e-01\n 8.97188131e-01 9.95954021e-01 1.00003336e+00 9.52937378e-01\n 1.03588225e+00 8.07229053e-01 9.45870936e-01 1.14285712e+00\n 1.05888891e+00 1.01351685e+00 9.95933245e-01 1.00595428e+00\n 1.00498029e+00 9.75791616e-01 1.00440288e+00 9.26551722e-01\n 1.09738081e+00 8.19107361e-01 1.03631795e+00 9.91082564e-01\n 9.91082564e-01 9.88737651e-01 9.88737651e-01 1.04419087e+00\n 1.04419087e+00 9.89731989e-01 9.94717198e-01 9.99627250e-01\n 9.83196840e-01 1.01837202e+00 1.02093226e+00 1.42829699e+00\n 1.14246095e+00 1.12316236e+00 1.62458315e+00 1.12097616e+00\n 1.09090905e+00 7.06589090e-01 1.06114270e+00 2.44209650e+00\n 8.48843919e-01 1.35615053e+00 1.00003828e+00 1.92910462e+00\n 1.19995387e+00 7.43776991e-01 8.66966662e-01 1.33261526e+00\n 1.22519297e+00 9.63073949e-01 1.02299505e+00 1.10336336e+00\n 1.05263152e+00 4.61804483e-01 1.09074103e+00 8.00122889e-01\n 6.66897184e-01 1.23039051e+00 8.89059661e-01 4.53352734e-01\n 8.13705402e-31 9.23254329e-01 6.89920181e-01 8.00092152e-01\n 5.00172846e-01 9.23147800e-01 9.30483144e-01 1.06666662e+00\n 4.28851408e-01 2.10655719e-01 2.83620268e-31]",
      "\n",
      "*** X_test *** \nmean: \n[ 0.00255838 -0.01137029 -0.00226762 -0.00218291  0.012492   -0.02136284\n  0.01254958  0.01094719 -0.00514594 -0.00113254  0.0091464  -0.0030944\n -0.00303599  0.01424645 -0.00221302 -0.01266296 -0.0097061   0.01040613\n -0.00218313 -0.00599015  0.01801805  0.004485   -0.0159563  -0.00099312\n -0.00316938 -0.00491745 -0.00676125 -0.00085651  0.01995752 -0.00511534\n -0.00217739 -0.01704928  0.00323953  0.00775319  0.00477351  0.004485\n -0.00599015 -0.00309037 -0.00091907 -0.01197405 -0.00593445  0.01367873\n -0.00286056  0.00958067  0.00240372  0.00642717  0.0188111  -0.00708852\n -0.00499074  0.01094092 -0.00348829 -0.00872667  0.00060071  0.00462662\n -0.00845474 -0.00209179 -0.00985235  0.01382998  0.00256451  0.00608906\n -0.0032877  -0.00215181 -0.01129037  0.01873857  0.01257825 -0.01066418\n  0.00610502 -0.01648163  0.0041423  -0.00373576  0.00242577  0.00215216\n -0.00215216  0.00808341 -0.00808341 -0.00306654  0.00306654  0.0021465\n  0.00509037 -0.0033607  -0.00051818 -0.00064836  0.01381118  0.000662\n -0.00800974  0.01512975  0.00776153  0.0020392   0.00635238 -0.00228994\n -0.00088694  0.02045142 -0.00763729  0.01591255 -0.00512578 -0.00391473\n  0.00235113 -0.00567749 -0.00704807 -0.00166489 -0.01733273 -0.01991971\n -0.00531298  0.02187135  0.00255884 -0.01410279 -0.00184096 -0.01120194\n  0.00404822 -0.01648235  0.01405258 -0.00971293 -0.00584451 -0.01169779\n -0.00575659  0.008622    0.00495756 -0.0030944  -0.00508733 -0.00352099\n -0.00468683 -0.00426639 -0.00619602] \nvariance: \n[1.00390139e+00 9.80976285e-01 9.96792566e-01 9.96629281e-01\n 1.01702300e+00 1.01800658e+00 1.03941191e+00 1.05655769e+00\n 9.71548157e-01 9.95976827e-01 1.04276338e+00 8.61578187e-01\n 8.00018438e-01 1.02115305e+00 9.96700160e-01 9.80754586e-01\n 9.85500627e-01 1.01572041e+00 9.96050403e-01 9.91983948e-01\n 1.08974158e+00 1.00338845e+00 8.83934585e-01 9.94804433e-01\n 9.85583845e-01 9.61364162e-01 9.53497519e-01 9.92875719e-01\n 1.07933027e+00 9.29734774e-01 9.87624686e-01 8.53415072e-01\n 1.03113419e+00 1.19029487e+00 1.01043371e+00 1.00338845e+00\n 9.91983948e-01 9.90044364e-01 9.98930661e-01 9.97967023e-01\n 9.87401877e-01 1.00987889e+00 9.84650839e-01 1.05213419e+00\n 1.02097543e+00 1.24420546e+00 1.10589308e+00 9.83971343e-01\n 9.86803799e-01 1.02707543e+00 9.92088006e-01 9.80457318e-01\n 1.00270636e+00 1.01686147e+00 9.79623277e-01 9.88639819e-01\n 9.58576418e-01 1.19882462e+00 1.01750694e+00 1.37126008e+00\n 9.86011939e-01 9.95895745e-01 9.95559620e-01 1.02076060e+00\n 1.06902994e+00 9.72663831e-01 9.87692032e-01 9.12675980e-01\n 1.04217898e+00 9.60312638e-01 1.00668724e+00 1.00153140e+00\n 1.00153140e+00 9.75593462e-01 9.75593462e-01 1.01324567e+00\n 1.01324567e+00 1.00372851e+00 1.01760111e+00 9.99552495e-01\n 9.98386328e-01 9.99095945e-01 9.63891107e-01 1.02853195e+00\n 8.46046423e-01 1.25728319e+00 1.13435064e+00 1.03161681e+00\n 1.30888984e+00 9.60103667e-01 9.79606991e-01 1.77678078e+00\n 8.48791894e-01 1.34183441e+00 9.05493305e-01 8.82853654e-01\n 1.11992260e+00 9.29743731e-01 8.53602653e-01 9.60044375e-01\n 6.45692619e-01 5.63461095e-01 9.63201898e-01 1.65392577e+00\n 1.09463595e+00 5.54080281e-01 9.60059181e-01 5.33464452e-01\n 1.13316791e+00 5.74712313e-01 1.37702580e+00 7.85231807e-01\n 7.38540968e-01 7.38830862e-01 8.27726188e-01 1.43972353e+00\n 1.19982488e+00 8.61578187e-01 9.11873822e-01 8.53385797e-01\n 8.57254891e-01 8.42182931e-01 1.44327384e-31]",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from simple_ml.preprocessing import Standardizer\n",
    "\n",
    "print('Before normalized the data...')\n",
    "print('*** X_train *** \\nmean: \\n%s \\nvariance: \\n%s' % (np.mean(X_train, axis=0), np.var(X_train, axis=0)))\n",
    "print('*** X_val *** \\nmean: \\n%s \\nvariance: \\n%s' % (np.mean(X_val, axis=0), np.var(X_val, axis=0)))\n",
    "print('*** X_test *** \\nmean: \\n%s \\nvariance: \\n%s' % (np.mean(X_test, axis=0), np.var(X_test, axis=0)))\n",
    "standardizer = Standardizer()\n",
    "standardizer.fit(X_train)\n",
    "X_train = standardizer.transform(X_train)\n",
    "X_val = standardizer.transform(X_val)\n",
    "X_test = standardizer.transform(X_test)\n",
    "\n",
    "print('\\nAfter normalized the data...')\n",
    "print('*** X_train *** \\nmean: \\n%s \\nvariance: \\n%s' % (np.mean(X_train, axis=0), np.var(X_train, axis=0)))\n",
    "print('*** X_val *** \\nmean: \\n%s \\nvariance: \\n%s' % (np.mean(X_val, axis=0), np.var(X_val, axis=0)))\n",
    "print('*** X_test *** \\nmean: \\n%s \\nvariance: \\n%s' % (np.mean(X_test, axis=0), np.var(X_test, axis=0)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression\n",
    "We first recap the math of Logistic Regression.\n",
    "Given $$ \\textbf{X} \\in \\mathbb{R}^{m \\times n} $$, where m is the number of examples, and n is the number of features;\n",
    "\n",
    "And learnable parameters $$ \\textbf{w} \\in \\mathbb{R}^{n \\times h}$$, $$ \\textbf{b} \\in \\mathbb{R}^{h}$$.\n",
    "\n",
    "Linear Model is $$ \\textbf{y} = \\textbf{X} \\cdot \\textbf{w} + \\textbf{b} $$.\n",
    "\n",
    "We can add a column of ones in $$ \\textbf{X} $$, so it become fully Vectorized form, because we concate $$ \\textbf{w} $$ and $$ \\textbf{b} $$ together. \n",
    "\n",
    "So we get $$ \\textbf{X} \\in \\mathbb{R}^{m \\times (n + 1)}$$ and learnable parameter $$ \\textbf{W} \\in \\mathbb{R}^{(n + 1) \\times h}$$\n",
    "\n",
    "Then we make the derivation of Closed Form Solution.\n",
    "For Linear Regression problem, we want to minimize the loss function $$ J(\\hat{y}, y) = \\frac{1}{2m}\\Sigma_{i=1}^{m}(\\hat{y}_{i} - y_{i})  $$\n",
    "\n",
    "where $$ \\hat{y} =  \\textbf{X} \\cdot \\textbf{W} \\in \\mathbb{R}^{m \\times h}$$ is prediction output by the Linear Model.\n",
    "\n",
    "We can write it as Vectorized form as \\begin{eqnarray} J(\\hat{y}, y) &=& \\frac{1}{2m}(\\hat{y} - y)^{T} \\cdot (\\hat{y} - y) \\\\\n",
    "&=& \\frac{1}{2m}(\\textbf{X} \\textbf{W} - y)^{T} \\cdot (\\textbf{X} \\textbf{W} - y) \\\\\n",
    "\\textbf{W}^{*} &=& \\mathop{\\arg\\min}_{W} J(\\hat{y}, y) \\end{eqnarray} \n",
    "\n",
    "Set $$ \\frac{\\partial{J}}{\\partial{W}} = 0 $$, we get $$ \\textbf{W}^{*} = (\\textbf{X}^{T}\\textbf{X})^{-1}\\textbf{X}^{T}\\textbf{y} $$\n",
    "Note that $$ (\\textbf{X}^{T}\\textbf{X}) $$ maybe uninvertible, so in practice, we use pesudo inverse or Ridge Regression.\n",
    "\n",
    "Now, it's time to get your hands dirty!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we first build the linear regression model.\n",
    "\n",
    "# build the linear model\n",
    "print(10 * '*' + ' LinearRegression model begin ' + 10 * '*')\n",
    "linear_regression = LinearRegression()\n",
    "# get closed form solution\n",
    "linear_regression.fit(X_train, y_train)\n",
    "test_y_hat = linear_regression.predict(X_test)\n",
    "train_y_hat = linear_regression.predict(X_train)\n",
    "training_error = square_error(y_train, train_y_hat) / y_train.shape[0]\n",
    "test_error = square_error(y_test, test_y_hat) / y_test.shape[0]\n",
    "print('Training mean square error: ', training_error)\n",
    "print('Test mean square error: ', test_error)\n",
    "print(10 * '*' + ' LinearRegression model end ' + 10 * '*')\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ridge Regression\n",
    "Ridge Regression model can be seen as a regularized Linear Model.\n",
    "\n",
    "And it doesn't have any un invertible problem when getting closed form solution.\n",
    "\n",
    "The closed form solution of ridge regression is computed as following.\n",
    "$$ \\textbf{W}^{*} = (\\textbf{X}^{T}\\textbf{X} + \\lambda \\mathbb{I})^{-1}\\textbf{X}^{T}\\textbf{y} $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build the ridge regression model\n",
    "print(10 * '*' + ' Ridge LinearRegression model begin ' + 10 * '*')\n",
    "# the lambda is represented as alpha in the API, by default, alpha=1\n",
    "ridge_regression = RidgeRegression(alpha=1)\n",
    "# get closed form solution\n",
    "ridge_regression.fit(X_train, y_train)\n",
    "test_y_hat = ridge_regression.predict(X_test)\n",
    "train_y_hat = ridge_regression.predict(X_train)\n",
    "training_error = square_error(y_train, train_y_hat) / y_train.shape[0]\n",
    "test_error = square_error(y_test, test_y_hat) / y_test.shape[0]\n",
    "print('Training mean square error: ', training_error)\n",
    "print('Test mean square error: ', test_error)\n",
    "print(10 * '*' + ' Ridge LinearRegression model end ' + 10 * '*')\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Different lambdas in Ridge Regression\n",
    "Now we explore the different alpha's impact in Ridge Regression.\n",
    "\n",
    "We explore it in some magnitude, from 1e-7 to 1e3.\n",
    "\n",
    "We can see that, when the lambda is small, e.g., from 1e-7 to 10, the both training error and validation error is very small.\n",
    "But when lambda become larger than 10, both error become very large."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "magnitude_range = [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3]\n",
    "lambdas = [1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]\n",
    "training_errors = []\n",
    "validation_errors = []\n",
    "for l in tqdm(lambdas):\n",
    "    ridge_regression = RidgeRegression(alpha=l)\n",
    "    # get closed form solution\n",
    "    ridge_regression.fit(X_train, y_train)\n",
    "    val_y_hat = ridge_regression.predict(X_val)\n",
    "    train_y_hat = ridge_regression.predict(X_train)\n",
    "    training_error = square_error(y_train, train_y_hat) / y_train.shape[0]\n",
    "    val_error = square_error(y_val, val_y_hat) / y_val.shape[0]\n",
    "    training_errors.append(training_error)\n",
    "    validation_errors.append(val_error)\n",
    "\n",
    "    \n",
    "plt.plot(magnitude_range, np.array(training_errors), label='training-error')\n",
    "plt.plot(magnitude_range, np.array(validation_errors), label='validation-error')\n",
    "plt.xlabel('lambda magnitude')\n",
    "plt.ylabel('error')\n",
    "plt.xticks(magnitude_range)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"In validation set, the min error occurs when lambda is \", lambdas[np.argmin(validation_errors)], '.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test best lambda's performance.\n",
    "Now we test the best lambda's performance on test set. Note that all the hyper-parameter tuning must perform on validation set or training set.\n",
    "And we test the final result on the test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(10 * '*' + ' Best Ridge LinearRegression model begin ' + 10 * '*')\n",
    "# the lambda is represented as alpha in the API, by default, alpha=1\n",
    "ridge_regression = RidgeRegression(alpha=1e-7)\n",
    "# get closed form solution\n",
    "ridge_regression.fit(X_train, y_train)\n",
    "test_y_hat = ridge_regression.predict(X_test)\n",
    "train_y_hat = ridge_regression.predict(X_train)\n",
    "training_error = square_error(y_train, train_y_hat) / y_train.shape[0]\n",
    "test_error = square_error(y_test, test_y_hat) / y_test.shape[0]\n",
    "print('Training mean square error: ', training_error)\n",
    "print('Test mean square error: ', test_error)\n",
    "print(10 * '*' + ' Best Ridge LinearRegression model end ' + 10 * '*')\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression: Optimize it using Gradient Descent\n",
    "\n",
    "Recap: \n",
    "\n",
    "We have Linear model: $$ \\hat{y} =  \\textbf{X} \\cdot \\textbf{W} $$, where $$ \\hat{y} \\in \\mathbb{R}^{m \\times h}, \\textbf{X} \\in \\mathbb{R}^{m \\times (n+1)}, \\mathbb{W}^{(n+1) \\times h} $$\n",
    "\n",
    "We have Loss function (Least Square Loss): \n",
    "\\begin{eqnarray} J(\\hat{y}, y) &=& \\frac{1}{2m}(\\hat{y} - y)^{T} \\cdot (\\hat{y} - y) \\\\\n",
    "&=& \\frac{1}{2m}(\\textbf{X} \\textbf{W} - y)^{T} \\cdot (\\textbf{X} \\textbf{W} - y) \\end{eqnarray} \n",
    "\n",
    "The Gradient w.r.t $$\\textbf{W}$$ is:\n",
    " \n",
    " \\begin{eqnarray} \\frac{\\partial J}{W} &=& \\frac{1}{2m} \\frac{\\partial J}{\\hat{y}} \\frac{\\partial \\hat{y}}{W} \\\\\n",
    " &=&  \\frac{1}{m} X^{T}(\\textbf{X} \\textbf{W} - y)   \\end{eqnarray}\n",
    " \n",
    "With learning rate $$\\eta \\in \\mathbb{R} $$, which typically is a very small float number, e.g., 1e-3, 1e-4...\n",
    " \n",
    "Gradient Descent Update the $$\\textbf{W}$$ using the following equation.\n",
    "\n",
    "$$ \\textbf{W}_{t+1} = \\textbf{W}_t - \\eta \\frac{\\partial J}{W} $$ \n",
    "\n",
    "But all of the details are warp as A Layer for you to call in simple_ml.\n",
    "\n",
    "Note that, In Gradient descent, we use all the training data to update the parameter in each run.\n",
    "\n",
    "We can see that, using the whole data to update typically need more iterations to fully converge, comparing with Stochastic\n",
    "Gradient Descent(SGD) or Mini Batch Gradient Descent(MBGD).\n",
    "But GD fully use the vectorization computation, which make it super fast, you can see that, the whole process uses less \n",
    "than 1 seconds."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(10 * '*' + ' GD Linear model ' + 10 * '*')\n",
    "\n",
    "# build the linear model with gradient descent\n",
    "# define layer\n",
    "Inputs = Input(input_shape=X_train.shape[1])\n",
    "linear_out = Linear(output_dim=1, activation=None, initializer=ones)(Inputs)\n",
    "model = Model(Inputs, linear_out)\n",
    "model.compile('MSE', optimizer=SGD(lr=0.01))\n",
    "begin = datetime.now()\n",
    "model.fit(X_train, y_train,\n",
    "          verbose=100, epochs=500,\n",
    "          validation_data=(X_val, y_val),\n",
    "          batch_size=X_train.shape[0], # here we set batch_size \n",
    "                                       # equal to number of training examples \n",
    "                                       # to implement Gradient Descent.\n",
    "                                \n",
    "          metric='MAE',\n",
    "          shuffle=False,\n",
    "          # peek_type='single-reg'\n",
    "          )\n",
    "end = datetime.now()\n",
    "plt.subplot(211)\n",
    "plt.plot(model.train_losses, label='train_losses')\n",
    "plt.plot(model.validation_losses, label='valid_losses')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.subplot(212)\n",
    "plt.plot(model.train_metrics, label='train_metrics')\n",
    "plt.plot(model.validation_metrics, label='valid_metrics')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "train_y_hat = model.forward(X_train)\n",
    "test_y_hat = model.forward(X_test)\n",
    "training_error = square_error(y_train, train_y_hat) / y_train.shape[0]\n",
    "test_error = square_error(y_test, test_y_hat) / y_test.shape[0]\n",
    "\n",
    "print('Training error: ', training_error)\n",
    "print('Test error: ', test_error)\n",
    "print('Cost: ',  (end - begin), ' seconds')\n",
    "print(10 * '*' + ' GD Linear model end ' + 10 * '*')\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression: Optimize it using Stochastic Gradient Descent(SGD)\n",
    "\n",
    "Note that, In Stochastic Gradient descent, we use one training example to update the parameter in each run.\n",
    "\n",
    "We can see that, SGD brings fast convergence compared with GD.\n",
    "But SGD hasn't use the vectorization computation at all, which make it super slow, you can see that, the whole process uses 25x \n",
    "times more than GD."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(10 * '*' + ' SGD Linear model ' + 10 * '*')\n",
    "\n",
    "# build the linear model with gradient descent\n",
    "# define layer\n",
    "Inputs = Input(input_shape=X_train.shape[1])\n",
    "linear_out = Linear(output_dim=1, activation=None, initializer=ones)(Inputs)\n",
    "model = Model(Inputs, linear_out)\n",
    "model.compile('MSE', optimizer=SGD(lr=0.01))\n",
    "begin = datetime.now()\n",
    "model.fit(X_train, y_train,\n",
    "          verbose=100, epochs=500,\n",
    "          validation_data=(X_val, y_val),\n",
    "          batch_size=1, # here we set batch_size = 1 \n",
    "                        # to implement Stochastic Gradient Descent.\n",
    "                                \n",
    "          metric='MAE',\n",
    "          shuffle=False,\n",
    "          # peek_type='single-reg'\n",
    "          )\n",
    "end = datetime.now()\n",
    "plt.subplot(211)\n",
    "plt.plot(model.train_losses, label='train_losses')\n",
    "plt.plot(model.validation_losses, label='valid_losses')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.subplot(212)\n",
    "plt.plot(model.train_metrics, label='train_metrics')\n",
    "plt.plot(model.validation_metrics, label='valid_metrics')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "train_y_hat = model.forward(X_train)\n",
    "test_y_hat = model.forward(X_test)\n",
    "training_error = square_error(y_train, train_y_hat) / y_train.shape[0]\n",
    "test_error = square_error(y_test, test_y_hat) / y_test.shape[0]\n",
    "\n",
    "print('Training error: ', training_error)\n",
    "print('Test error: ', test_error)\n",
    "print('Cost: ',  (end - begin), ' seconds')\n",
    "print(10 * '*' + ' SGD Linear model end ' + 10 * '*')\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression: Optimize it using Mini Batch Gradient Descent(MBGD)\n",
    "\n",
    "In the above comparision between GD and SGD, we can clearly see that:\n",
    "\n",
    "|      Metric     |       GD      |      SGD      |\n",
    "| --------------- | ------------- | ------------- |\n",
    "| Time-Efficiency |       Yse     |       No      |\n",
    "| Convergence     |      Slow     |      Fast     |\n",
    "\n",
    "Clearly there is a trade off to be made. But how can we balance the trade off ?\n",
    "\n",
    "The key is to control the batch_size.\n",
    "- GD: batch_size: all examples\n",
    "- SGD: batch_size: 1\n",
    "- MBGD: batch_size: between 1 and all examples, typically 16, 32, 64...\n",
    "\n",
    "Now we compare the influence w.r.t time efficiency and convergence.\n",
    "\n",
    "As stated before, GD and MBGD both have its metric and drawback, what we can do is making trade off.\n",
    "As illustrated by the following three graphs, the smaller the batch_size, the faster for convergence,\n",
    "but cost more time. So in practice, we set batch_size bigger than one, but smaller than the number of \n",
    "examples, typically 32, 64, 128..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_sizes = [1, 8, 16, 32, 64, 128, X_train.shape[0]]\n",
    "batch_size2training_losses = {b: None for b in batch_sizes}\n",
    "batch_size2validation_losses = {b: None for b in batch_sizes}\n",
    "batch_size2timecost = []\n",
    "\n",
    "for b in batch_sizes:\n",
    "    print('Starting with batch_size = %d' % b)\n",
    "    Inputs = Input(input_shape=X_train.shape[1])\n",
    "    linear_out = Linear(output_dim=1, activation=None, initializer=ones)(Inputs)\n",
    "    model = Model(Inputs, linear_out)\n",
    "    model.compile('MSE', optimizer=SGD(lr=0.01))\n",
    "    begin = datetime.now()\n",
    "    model.fit(X_train, y_train,\n",
    "              verbose=100, epochs=500,\n",
    "              validation_data=(X_val, y_val),\n",
    "              batch_size=b, # here we set batch_size with a bunch of sizes \n",
    "                            # to implement both SGD, MBGD and GD               \n",
    "              metric='MAE',\n",
    "              shuffle=False,\n",
    "              # peek_type='single-reg'\n",
    "              )\n",
    "    end = datetime.now()\n",
    "    cost = (end - begin).seconds\n",
    "    batch_size2training_losses[b] = model.train_losses\n",
    "    batch_size2validation_losses[b] = model.valid_losses\n",
    "    batch_size2timecost.append(cost)\n",
    "    print('End with batch_size = %d\\n' % b)\n",
    "\n",
    "for b in batch_sizes:\n",
    "    plt.plot(batch_size2training_losses[b], label='batch_size = %d' % b)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('$MSE_{train}$')\n",
    "plt.legend()\n",
    "plt.savefig('batch_size_train.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "for b in batch_sizes:\n",
    "    plt.plot(batch_size2validation_losses[b], label='batch_size = %d' % b)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('$MSE_{val}$')\n",
    "plt.legend()\n",
    "plt.savefig('batch_size_val.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(batch_sizes, batch_size2timecost, marker='v')\n",
    "plt.xlabel('batch_size')\n",
    "plt.ylabel('time cost (Seconds)')\n",
    "plt.grid()\n",
    "plt.savefig('time_cost.png', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tuning Hyper-Parameter\n",
    "Now it's time to tuning the hyper-parameter of Gradient Descent Algorithm.\n",
    "In the simple GD, the only hyper-parameter is learning rate $$\\eta$$.\n",
    "\n",
    "Now we validate the best $$ \\eta $$ in validation set and test our final model.\n",
    "We first search it in log space, e.g., 1e-4, 1e-3, 1e-2, 1e-1, 1.\n",
    "As Bigger learning rate will lead to very big loss, we do not try it here.\n",
    "As seen below, the best learning rate is 1e-2.\n",
    "\n",
    "The bigger the learning rate, the faster the loss decreases, but it may not converge to a very low loss.\n",
    "\n",
    "The smaller the learning rate, it may converge to a very low loss, but the loss decrease very slowly. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rates = [1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print('Starting with learning_rate = %.4f' % lr)\n",
    "    Inputs = Input(input_shape=X_train.shape[1])\n",
    "    linear_out = Linear(output_dim=1, activation=None, initializer=ones)(Inputs)\n",
    "    model = Model(Inputs, linear_out)\n",
    "    model.compile('MSE', optimizer=SGD(lr=lr))\n",
    "    model.fit(X_train, y_train,\n",
    "              verbose=200, epochs=200, # we set epochs to 1000 for fully convergence\n",
    "              validation_data=(X_val, y_val),\n",
    "              batch_size=64, # here we set batch_size to 64 considering the trade off            \n",
    "              metric='MAE',\n",
    "              shuffle=False,\n",
    "              # peek_type='single-reg'\n",
    "              )\n",
    "    training_losses.append(model.training_losses)\n",
    "    validation_losses.append(model.valid_losses)\n",
    "    print('End with learning_rate = %.4f\\n' % lr)\n",
    "\n",
    "for lr, train_loss in zip(learning_rates, training_losses):\n",
    "    plt.plot(train_loss, label='lr = %1.0e' % lr)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('$MSE_{train}$')\n",
    "plt.legend(loc='upper right', prop = {'size':7})\n",
    "plt.savefig('lr_train.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "for lr, val_loss in zip(learning_rates, validation_losses):\n",
    "    plt.plot(val_loss, label='lr = %1.0e' % lr)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('$MSE_{val}$')\n",
    "plt.legend(loc='upper right', prop = {'size':8})\n",
    "plt.savefig('lr_val.png', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Difference of $$\\textbf{W}$$\n",
    "Now we explore the differences of $$\\textbf{W}$$ in Closed Form Solution, Ridge Regression and MBGD.\n",
    "\n",
    "More specifically, we will evaluate the $$l_2norm$$ of $$\\textbf{W}$$ under different setting.\n",
    "\n",
    "Given $$ \\textbf{x} \\in \\mathbb{R}^{n} $$, $$ l_2norm(\\textbf{x}) = \\sqrt{\\Sigma_{i=1}^{n}\\textbf{x}_{i}^{2}}$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "magnitude_range = [-7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3]\n",
    "lambdas = [1e-7,1e-6,1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]\n",
    "\n",
    "linear_Ws = len(lambdas) * [np.linalg.norm(linear_regression.beta[1:], ord=2, axis=1)[0]]\n",
    "ridge_Ws = []\n",
    "\n",
    "for l in tqdm(lambdas):\n",
    "    ridge_regression = RidgeRegression(alpha=l)\n",
    "    # get closed form solution\n",
    "    ridge_regression.fit(X_train, y_train)\n",
    "    ridge_Ws.append(np.linalg.norm(ridge_regression.beta[1:], ord=2, axis=1)[0])\n",
    "\n",
    "    \n",
    "plt.plot(magnitude_range, np.array(linear_Ws), label='linear regression', marker='v')\n",
    "plt.plot(magnitude_range, np.array(ridge_Ws), label='ridge regression', marker='v')\n",
    "plt.xlabel('lambda magnitude')\n",
    "plt.ylabel('$W$ magnitude')\n",
    "plt.xticks(magnitude_range)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('ridge_w.png', dpi=300)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train, y_train)\n",
    "\n",
    "NUM_EPOCH = 500\n",
    "\n",
    "linear_Ws = NUM_EPOCH * [np.linalg.norm(linear_regression.beta[1:], ord=2, axis=1)[0]]\n",
    "\n",
    "MBGD_Ws = []\n",
    "Inputs = Input(input_shape=X_train.shape[1])\n",
    "linear_out = Linear(output_dim=1, activation=None)(Inputs)\n",
    "MBGD_linear_regression = Model(Inputs, linear_out)\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    MBGD_linear_regression.compile('MSE', optimizer=SGD(lr=0.01)) # use the best lambda here.\n",
    "    MBGD_linear_regression.fit(X_train, y_train,\n",
    "              verbose=-1, epochs=1, # we set epochs to 1000 for fully convergence\n",
    "              validation_data=(X_val, y_val),\n",
    "              batch_size=64, # here we set batch_size to 64 considering the trade off            \n",
    "              metric='MAE',\n",
    "              shuffle=False)\n",
    "    MBGD_Ws.append(np.linalg.norm(linear_out.weight.T, ord=2, axis=1)[0])\n",
    "    \n",
    "plt.plot(linear_Ws, label='CFS')\n",
    "plt.plot(MBGD_Ws, label='MBGD')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('$W$ magnitude')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.legend( loc='upper right')\n",
    "plt.savefig('mbgd_w.png', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Performance\n",
    "In this section, we report the performance on CFS optimized LR, RR and MBGD optimized LR in training set, validation set and test set. \n",
    "The $\\lambda$ is set to 1e-7 after validation. And batch size is set to 64 in MBGD, learing rate set to 0.01 and epoch is set to 200 by validation.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(10 * '*' + ' Best LinearRegression model ' + 10 * '*')\n",
    "# the lambda is represented as alpha in the API, by default, alpha=1\n",
    "linear_regression = LinearRegression()\n",
    "# get closed form solution\n",
    "linear_regression.fit(X_train, y_train)\n",
    "test_y_hat = linear_regression.predict(X_test)\n",
    "val_y_hat = linear_regression.predict(X_val)\n",
    "train_y_hat = linear_regression.predict(X_train)\n",
    "training_error = square_error(y_train, train_y_hat) / y_train.shape[0]\n",
    "val_error = square_error(y_val, val_y_hat) / y_val.shape[0]\n",
    "test_error = square_error(y_test, test_y_hat) / y_test.shape[0]\n",
    "print('Training mean square error: ', training_error)\n",
    "print('Val mean square error: ', val_error)\n",
    "print('Test mean square error: ', test_error)\n",
    "print(10 * '*' + ' Best LinearRegression model end ' + 10 * '*')\n",
    "print()\n",
    "\n",
    "print(10 * '*' + ' Best Ridge LinearRegression model ' + 10 * '*')\n",
    "# the lambda is represented as alpha in the API, by default, alpha=1\n",
    "ridge_regression = RidgeRegression(alpha=1e-7)\n",
    "# get closed form solution\n",
    "ridge_regression.fit(X_train, y_train)\n",
    "test_y_hat = ridge_regression.predict(X_test)\n",
    "val_y_hat = ridge_regression.predict(X_val)\n",
    "train_y_hat = ridge_regression.predict(X_train)\n",
    "training_error = square_error(y_train, train_y_hat) / y_train.shape[0]\n",
    "val_error = square_error(y_val, val_y_hat) / y_val.shape[0]\n",
    "test_error = square_error(y_test, test_y_hat) / y_test.shape[0]\n",
    "print('Training mean square error: ', training_error)\n",
    "print('Val mean square error: ', val_error)\n",
    "print('Test mean square error: ', test_error)\n",
    "print(10 * '*' + ' Best Ridge LinearRegression model end ' + 10 * '*')\n",
    "print()\n",
    "\n",
    "print(10 * '*' + ' Best MBGD Linear model ' + 10 * '*')\n",
    "# build the linear model with gradient descent\n",
    "# define layer\n",
    "Inputs = Input(input_shape=X_train.shape[1])\n",
    "linear_out = Linear(output_dim=1, activation=None, initializer=ones)(Inputs)\n",
    "model = Model(Inputs, linear_out)\n",
    "model.compile('MSE', optimizer=SGD(lr=0.01))\n",
    "model.fit(X_train, y_train,\n",
    "          verbose=-1, epochs=200,\n",
    "          validation_data=(X_val, y_val),\n",
    "          batch_size=64,             \n",
    "          metric='MAE',\n",
    "          shuffle=False,\n",
    "          # peek_type='single-reg'\n",
    "          )\n",
    "train_y_hat = model.forward(X_train)\n",
    "test_y_hat = model.forward(X_test)\n",
    "val_y_hat = model.forward(X_val)\n",
    "training_error = square_error(y_train, train_y_hat) / y_train.shape[0]\n",
    "test_error = square_error(y_test, test_y_hat) / y_test.shape[0]\n",
    "val_error = square_error(y_val, val_y_hat) / y_val.shape[0]\n",
    "\n",
    "print('Training error: ', training_error)\n",
    "print('Val mean square error: ', val_error)\n",
    "print('Test error: ', test_error)\n",
    "print(10 * '*' + ' Best MBGD Linear model end ' + 10 * '*')\n",
    "print()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}